from pyspark.sql import SparkSession
from pyspark.sql.types import *

#mostrar bancos de dados e tabelas
spark.sql("show databases").show()
#criar banco de dados
spark.sql("create database desp")
spark.sql("use desp").show()

#criar tabela gerenciada
arqschema = "id INT, nome STRING, status STRING, cidade STRING, vendas INT, data STRING"
despachantes = spark.read.csv("/home/ricardo/download/despachantes.csv", header=False, schema=arqschema)
despachantes.write.saveAsTable("Despachantes")

#mostra tabela
spark.sql("show tables").show()

#voltar ao nosso banco de dados
spark.sql("use desp").show()

# overwrite e append
despachantes.write.mode("overwrite").saveAsTable("Despachantes")

# gerar um dataframe
despachantes = spark.sql("select * from despachantes")
despachantes.show()

#criar tabela não gerenciada
despachantes.write.format("parquet").save("/home/ricardo/desparquet") 
despachantes.write.option("path", "/home/ricardo/desparquet").saveAsTable("Despachantes_ng")

spark.sql("show create table Despachantes").show(truncate=False)
#despachantes ng mostra, indicando que é não gerenciada
spark.sql("show create table Despachantes_ng").show(truncate=False)

#outra forma:
spark.catalog.listTables()

# Path tabela gerenciada
/home/ricardo/spark-warehouse/desp.db/despachantes

## views

despachantes.createOrReplaceTempView("Despachantes")
despachantes.createOrReplaceGlobalTempView("Despachantes")

## consultas

from pyspark.sql import functions as Func
from pyspark.sql.functions import sum

#mostrar a tabela
spark.sql("Select * from Despachantes").show()
despachantes.show()

#mostrar certas colunas
spark.sql("Select nome,vendas from Despachantes").show()
despachantes.select("nome","vendas").show()

#condição lógica
spark.sql("Select nome,vendas from Despachantes where vendas > 20").show()
despachantes.select("id","nome","vendas").where(Func.col("vendas") > 20).show()

spark.sql("Select cidade,sum(vendas) from Despachantes group by cidade order by 2 desc").show()
despachantes.groupBy("cidade").agg(sum("vendas")).orderBy(Func.col("sum(vendas)").desc()).show()

##

recschema = "idrec INT, datarec STRING, iddesp INT"
reclamacoes = spark.read.csv("/home/ricardo/download/reclamacoes.csv", header=False, schema=recschema)
reclamacoes.write.saveAsTable("reclamacoes")

#inner join
spark.sql("select reclamacoes.*, despachantes.nome from despachantes inner join reclamacoes  on (despachantes.id = reclamacoes.iddesp)").show()

#righ join deve trazer o mesmo resultado, pois todas as reclamações tem um despachante
spark.sql("select reclamacoes.*, despachantes.nome from despachantes right join reclamacoes  
on (despachantes.id = reclamacoes.iddesp)").show()

#left join traz mais colunas e campos nulos, pois alguns despachantes não tem reclações
spark.sql("select reclamacoes.*, despachantes.nome from despachantes left join reclamacoes  on (despachantes.id = reclamacoes.iddesp)").show()

#inner join
despachantes.join(reclamacoes,despachantes.id == reclamacoes.iddesp, "inner").select("idrec","datarec","iddesp","nome").show()

#right
despachantes.join(reclamacoes,despachantes.id == reclamacoes.iddesp, "right").select("idrec","datarec","iddesp","nome").show()

#left
despachantes.join(reclamacoes,despachantes.id == reclamacoes.iddesp, "right").select("idrec","datarec","iddesp","nome").show()

##spark-sql
show databases;
use desp;
show tables;

Select * from Despachantes;

Select nome,vendas from Despachantes;

#condição lógica
Select nome,vendas from Despachantes where vendas > 20;

#agrupar
Select cidade,sum(vendas) from Despachantes group by cidade order by 2 desc;

join
select reclamacoes.*, despachantes.nome from despachantes inner join reclamacoes  on (despachantes.id = reclamacoes.iddesp);